{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuradha1105/RAG-Assignment/blob/main/RAG_Quickstart_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc1f377",
      "metadata": {
        "id": "cdc1f377"
      },
      "source": [
        "\n",
        "# Retrieval-Augmented Generation (RAG) ‚Äî Zero-to-One Notebook\n",
        "**Stack:** LangChain ‚Ä¢ OpenAI (swap-friendly) ‚Ä¢ Chroma (FAISS optional)\n",
        "\n",
        "This is a **step-by-step** notebook for absolute beginners. Run each cell **top to bottom**.  \n",
        "It creates a tiny sample knowledge base so you can get a working demo **without any extra files**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8f111b0",
      "metadata": {
        "id": "d8f111b0"
      },
      "source": [
        "\n",
        "## ‚úÖ What you'll do\n",
        "1. Install libraries  \n",
        "2. Set API keys  \n",
        "3. Create sample documents (or point to your own)  \n",
        "4. Split into chunks  \n",
        "5. Build a **Chroma** vector store (FAISS optional)  \n",
        "6. Create a retriever + LLM chain  \n",
        "7. Ask grounded questions  \n",
        "8. (Optional) Try other providers (Claude/Cohere/Bedrock)\n",
        "\n",
        "**Screenshots:**\n",
        "- Installation success\n",
        "- Vector store summary (num docs/chunks)\n",
        "- First grounded answer\n",
        "- (Optional) FAISS run + answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0eda611",
      "metadata": {
        "id": "b0eda611"
      },
      "source": [
        "## 1) Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63fb3454",
      "metadata": {
        "id": "63fb3454"
      },
      "outputs": [],
      "source": [
        "\n",
        "# If in Colab, this will take ~1-2 minutes\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install \"langchain>=0.2.11\" \"langchain-community>=0.2.9\" \"langchain-openai>=0.1.7\"                \"chromadb>=0.5.3\" \"tiktoken>=0.7.0\" \"pypdf>=4.2.0\" \"faiss-cpu>=1.8.0.post1\"                \"langchain-anthropic>=0.1.19\" \"langchain-cohere>=0.1.9\"\n",
        "print(\"‚úÖ Installs complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae1dbdb",
      "metadata": {
        "id": "9ae1dbdb"
      },
      "source": [
        "\n",
        "## 2) Set your API key(s)\n",
        "\n",
        "- Default path uses **OpenAI** (you can swap later).  \n",
        "- Replace `\"paste-your-openai-key-here\"` with your real key from https://platform.openai.com/.  \n",
        "- For Claude/Cohere, uncomment and paste keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f4ec0cf",
      "metadata": {
        "id": "7f4ec0cf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"paste-your-openai-key-here\")\n",
        "\n",
        "# Optional: other providers\n",
        "# os.environ[\"ANTHROPIC_API_KEY\"] = \"paste-your-claude-key-here\"\n",
        "# os.environ[\"COHERE_API_KEY\"] = \"paste-your-cohere-key-here\"\n",
        "print(\"üîë Keys set (replace placeholders before running LLM cells).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502a87ee",
      "metadata": {
        "id": "502a87ee"
      },
      "source": [
        "\n",
        "## 3) Create a tiny sample knowledge base (you can also add your own .txt/.pdf later)\n",
        "\n",
        "If you add PDFs or TXT files to the `data_rag_demo/` folder, they'll be picked up automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e80db693",
      "metadata": {
        "id": "e80db693"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "DATA_DIR = Path(\"data_rag_demo\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "samples = {\n",
        "    \"rag_intro.txt\": \"\"\"Retrieval-Augmented Generation (RAG) couples information retrieval with text generation.\n",
        "The retriever pulls relevant chunks from a knowledge base; the LLM uses them to produce grounded answers.\"\"\",\n",
        "\n",
        "    \"vector_stores.txt\": \"\"\"Vector stores like Chroma and FAISS index embeddings so we can search by similarity.\n",
        "They store pairs of (vector, metadata) to find context quickly at query time.\"\"\",\n",
        "\n",
        "    \"best_practices.txt\": \"\"\"Good RAG practice: reasonable chunk sizes with overlap, clean text, track sources,\n",
        "persist your DB for reuse, and evaluate faithfulness & relevance of answers.\"\"\"\n",
        "}\n",
        "for name, text in samples.items():\n",
        "    (DATA_DIR / name).write_text(text, encoding=\"utf-8\")\n",
        "\n",
        "print(\"‚úÖ Sample files:\", [p.name for p in DATA_DIR.iterdir()])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ebf0275",
      "metadata": {
        "id": "1ebf0275"
      },
      "source": [
        "\n",
        "## 4) Load and split documents into chunks\n",
        "We use `RecursiveCharacterTextSplitter` which works well across formats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b264f110",
      "metadata": {
        "id": "b264f110"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "txt_loader = DirectoryLoader(str(DATA_DIR), glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True)\n",
        "docs_txt = txt_loader.load()\n",
        "\n",
        "pdf_loader = DirectoryLoader(str(DATA_DIR), glob=\"**/*.pdf\", loader_cls=PyPDFLoader, show_progress=True)\n",
        "docs_pdf = pdf_loader.load()\n",
        "\n",
        "docs = docs_txt + docs_pdf\n",
        "print(f\"üìÑ Loaded {len(docs)} documents\")\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "splits = splitter.split_documents(docs)\n",
        "print(f\"üî™ Created {len(splits)} chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e9e5030",
      "metadata": {
        "id": "3e9e5030"
      },
      "source": [
        "## 5) Build a Chroma vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f28172",
      "metadata": {
        "id": "b7f28172"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "PERSIST_DIR = \"chroma_db_demo\"\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=PERSIST_DIR)\n",
        "vectordb.persist()\n",
        "print(\"‚úÖ Chroma ready. Persist dir:\", PERSIST_DIR)\n",
        "try:\n",
        "    print(\"Collection count:\", vectordb._collection.count())\n",
        "except Exception as e:\n",
        "    print(\"Collection ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03650a0b",
      "metadata": {
        "id": "03650a0b"
      },
      "source": [
        "\n",
        "## 6) Create the retriever and the RAG chain\n",
        "We'll use a simple **stuff** chain: retrieved docs ‚Üí inserted into a prompt ‚Üí answered by the LLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bd4cf05",
      "metadata": {
        "id": "3bd4cf05"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a helpful assistant. Use ONLY the context below to answer. \"\n",
        "    \"If the answer isn't in the context, say you don't know.\\n\\n\"\n",
        "    \"Context:\\n{context}\\n\\nQuestion: {input}\"\n",
        ")\n",
        "doc_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, doc_chain)\n",
        "\n",
        "print(\"‚úÖ RAG chain is ready. Ask questions in the next cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8318a7e",
      "metadata": {
        "id": "d8318a7e"
      },
      "source": [
        "## 7) Ask a question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e7c8210",
      "metadata": {
        "id": "9e7c8210"
      },
      "outputs": [],
      "source": [
        "\n",
        "question = \"What is RAG and why do we use a vector store?\"\n",
        "result = rag_chain.invoke({\"input\": question})\n",
        "print(\"Q:\", question)\n",
        "print(\"\\nA:\", result[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6594b6ee",
      "metadata": {
        "id": "6594b6ee"
      },
      "source": [
        "## 8) (Optional) See retrieved sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dd81f7d",
      "metadata": {
        "id": "3dd81f7d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Inspect top retrieved docs for transparency\n",
        "docs = retriever.get_relevant_documents(\"Explain the purpose of vector stores in RAG.\")\n",
        "for i, d in enumerate(docs, 1):\n",
        "    print(f\"--- Source #{i} ---\")\n",
        "    print(\"Metadata:\", d.metadata)\n",
        "    print(d.page_content[:400], \"...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ea84a2a",
      "metadata": {
        "id": "8ea84a2a"
      },
      "source": [
        "\n",
        "## ‚≠ê Optional: Use FAISS instead of Chroma\n",
        "This block shows how to build a FAISS index. Run it if you want a second screenshot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b35faec9",
      "metadata": {
        "id": "b35faec9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "faiss_db = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "faiss_retriever = faiss_db.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "faiss_chain = create_retrieval_chain(faiss_retriever, doc_chain)\n",
        "ans = faiss_chain.invoke({\"input\": \"Give two best practices for RAG chunking\"})\n",
        "print(ans[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61b80088",
      "metadata": {
        "id": "61b80088"
      },
      "source": [
        "\n",
        "## ‚≠ê Optional: Try other LLM providers (Claude / Cohere / Bedrock)\n",
        "Uncomment the code and add your keys above to try alternative chat models.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}