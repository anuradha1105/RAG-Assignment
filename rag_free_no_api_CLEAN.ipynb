{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d4b042",
   "metadata": {
    "id": "b8d4b042"
   },
   "source": [
    "\n",
    "# Retrieval-Augmented Generation (RAG) ‚Äî Free / No API Key Version\n",
    "This notebook demonstrates a full **Retrieval-Augmented Generation (RAG)** pipeline using only free, local models.\n",
    "\n",
    "You‚Äôll:\n",
    "1. Create a small knowledge base  \n",
    "2. Chunk and embed text using Hugging Face (no API key)  \n",
    "3. Store and retrieve chunks with Chroma  \n",
    "4. Generate grounded answers with FLAN‚ÄëT5 (local model)\n",
    "\n",
    "**Screenshots:**  \n",
    "- Library install success  \n",
    "- Loaded docs / created chunks  \n",
    "- Vector store ready  \n",
    "- Final Q/A answer  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c77eb",
   "metadata": {
    "id": "e85c77eb"
   },
   "source": [
    "## 1Ô∏è‚É£ Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6922f05c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6922f05c",
    "outputId": "062f2256-929a-4a6b-c24a-c46e06f4d767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries installed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q langchain langchain-community chromadb sentence-transformers transformers accelerate pypdf\n",
    "print(\"‚úÖ All libraries installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5eeb95",
   "metadata": {
    "id": "bf5eeb95"
   },
   "source": [
    "## 2Ô∏è‚É£ Load and split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1666d675",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1666d675",
    "outputId": "066c8de3-8261-4d2f-ec4f-bd125cf446a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 3 docs ‚Üí üî™ Created 3 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "DATA_DIR = Path(\"data_rag_demo\")\n",
    "loader = DirectoryLoader(str(DATA_DIR), glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"üìÑ Loaded {len(docs)} docs ‚Üí üî™ Created {len(splits)} chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f22871",
   "metadata": {
    "id": "41f22871"
   },
   "source": [
    "## 3Ô∏è‚É£ Build local Chroma vector store (no API key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458008f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d458008f",
    "outputId": "db74958d-a34b-4de0-cff3-2971ac5e84f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Local Chroma vector store ready at: chroma_db_free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2581740092.py:8: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "PERSIST_DIR = \"chroma_db_free\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectordb = Chroma.from_documents(splits, embedding=embeddings, persist_directory=PERSIST_DIR)\n",
    "vectordb.persist()\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "print(\"‚úÖ Local Chroma vector store ready at:\", PERSIST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dSd8yS8hEg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8dSd8yS8hEg",
    "outputId": "6b32ee3c-fad6-4fdf-bf94-a3a64a0b0d07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ After unzip, I see: ['.config', 'chroma_db_free', 'RAG_Free_No_API.ipynb', 'RAG_file.zip', 'data_rag_demo', 'sample_data']\n",
      "üìÑ data_rag_demo contents: ['rag_intro.txt', 'best_practices.txt', 'vector_stores.txt']\n"
     ]
    }
   ],
   "source": [
    "import zipfile, os, shutil\n",
    "\n",
    "zip_name = \"RAG_file.zip\"   # must match the name shown in the left panel\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_name, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")\n",
    "\n",
    "print(\"üìÇ After unzip, I see:\", os.listdir(\".\"))\n",
    "\n",
    "# Move the data folder to current working directory if needed\n",
    "if os.path.exists(\"RAG_file/data_rag_demo\"):\n",
    "    shutil.move(\"RAG_file/data_rag_demo\", \".\")\n",
    "    print(\"‚úÖ Moved data_rag_demo into working directory\")\n",
    "\n",
    "print(\"üìÑ data_rag_demo contents:\", os.listdir(\"data_rag_demo\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w1T46Jn79IcT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1T46Jn79IcT",
    "outputId": "d51e87d0-de1e-48d3-8f76-8a1caf81ae1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 3 docs ‚Üí üî™ Created 3 chunks\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "DATA_DIR = Path(\"data_rag_demo\")\n",
    "\n",
    "loader = DirectoryLoader(str(DATA_DIR), glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"üìÑ Loaded {len(docs)} docs ‚Üí üî™ Created {len(splits)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1SCTWGJs3iAP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SCTWGJs3iAP",
    "outputId": "19368756-9cab-4fdc-ae08-5c3c475ca1ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 3\n",
      "Number of chunks: 3\n",
      "[DOC 0] source=data_rag_demo/rag_intro.txt preview='Retrieval-Augmented Generation (RAG) retrieves relevant context from a knowledge base and passes it to a language model '\n",
      "[DOC 1] source=data_rag_demo/best_practices.txt preview='Good RAG design uses chunk overlaps, stores metadata for traceability, and evaluates answers for faithfulness to the ret'\n",
      "[DOC 2] source=data_rag_demo/vector_stores.txt preview='Vector stores like Chroma store text embeddings so we can search by semantic meaning, not just exact keywords.'\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of docs:\", len(docs))\n",
    "print(\"Number of chunks:\", len(splits))\n",
    "\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"[DOC {i}] source={d.metadata.get('source')} preview={d.page_content[:120]!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef55074",
   "metadata": {
    "id": "5ef55074"
   },
   "source": [
    "## 4Ô∏è‚É£ Load free local model (FLAN‚ÄëT5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34988dbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259,
     "referenced_widgets": [
      "2757f8ef0979425f80fd570ef65dec8c",
      "62a7d01370e84d97aca2c71d16663192",
      "2dca15f4140f4d7f8bf63f7f126f059b",
      "5157413d0e664608b9d766515c2c0391",
      "8572843d772a4a9bbd107c2f6cae944c",
      "af352bb6c5be483e84cc4f2b9bb41478",
      "4bc5e91b07c54c4aacea283af728c127",
      "9f27046b210a4146b91af97657c85871",
      "f33c6d44ff1a433bab801d84a88bc027",
      "f0541e7c32bd432e909ec0b005c3104f",
      "2d74fd859d49495c88c903c5c792612d",
      "8a7abb913e854aedb2f9f0c549098f91",
      "2b68370b47f943aaa83f76da3f714209",
      "6225e632899f44ae8f3f5024152b2a1a",
      "56b588c110af4ef9b2e2fc2eea9c42a4",
      "e2207fcdda63426e87b9de74f014606b",
      "d841e8b8653145fa90d1b4f8f4fdbf5b",
      "57870bcf903b445993f9664035ceeb29",
      "811b82ec7a124ba5bcaa934516973e47",
      "78380a7818d54a17bdf592373aa71cf0",
      "dc3a71d5e44f4928bb67a58b1c3d4d1c",
      "56120d41f8654bf78b93534c611207db",
      "9333c939775c4ca4b58e84de10d6ad7d",
      "541b40be232f485d90edecd3259e6742",
      "ea22886827bc468ca238abcad2aad833",
      "d925f20802364d57a53856e2616edef5",
      "fce6da97356c4691b0fdea83fe6465f4",
      "bead9c98032343ef88a6813d3a1aa365",
      "7f2b98b8e5734eabbc692b443cb361a6",
      "e0b8c5339d994af2a6b299c942face74",
      "8165d6cf609d46a6b0b26631fe682d6c",
      "fa634b71a616464d9870a90b7f0688c1",
      "980c8dcae318402caf63f7bbad767199",
      "a2b38fc9f9f24418a6b669b8eb1806a8",
      "83abc2578d2b4024ba624838207ae8ec",
      "1c7f4010eb3e4ceda08a1212ac66651e",
      "a2d5dd1cafa442aca0f56cf5ce5a9a65",
      "634b43e782fb42b1b334289d9c77c466",
      "924437ac976a47508f2a1e8f7ceb26c4",
      "cda3cd5d635e462580ca493bca56bf26",
      "06943a3bb05448b5bf9e1ede6f2ae76e",
      "7197435d7d764721bccd6c5c662a758f",
      "41fa9bcb51a741a9ac4395d715a425ee",
      "c9fe5668ed1d4ce28654cf75217a2879",
      "e336038f80e1465baaaa80c52290dda2",
      "23598650f4814f5fa5126ff79312d6cf",
      "65bc6f493e534a7d9f04ef2f05b647b6",
      "e344a1ce3dab404081a29c6d48a44037",
      "a9b744914bb44d13a16a7bb57a369c34",
      "b9adda7710f34519a27e6c1f87f50365",
      "e74347f57dfa40578906884653772355",
      "ac160b4096074fac8fed4c589b59af38",
      "143163a504ff4f0ca9ea0754c7409fae",
      "244f4e04987a455ea3f0f1276175a0af",
      "d6446c4ac21c4162b250999684422d07",
      "7814dab679ec4c288b46aa7db7882300",
      "1d18e9b37ba64b7f96df0e1b04eca76e",
      "f974e5a26a994cc2a1e88f41be4870aa",
      "6b295af515874cdca8179232fc230673",
      "b3199bcb89414052986c162cfba8b69e",
      "1483155db766452e9d7067cf0d03fb01",
      "ed5bccfc4df345039b8c3f5f40fbdfb1",
      "fc183a3a5aef4748a52bf569e8368ea8",
      "0a1a2a82c5654060a982b42a15611fab",
      "cdbd1829e3244c8ebb54c60874415adf",
      "dc575cb38b0648369ba635317990994b",
      "7167dd7de51c4ca8b60bad9a8ba499a5",
      "4a9af0dd77c14279a41b3dc358edfab4",
      "c7edfc9615c44d56954500e22c606dd7",
      "f01d50d06c7c4895b52160a5f5a4b357",
      "0bdde48ee99341eaa1cf92c0f3551d2a",
      "1c1819c767324d3aac7c5d91820d2218",
      "ff0f6fda02f547dbba74ab109aa14695",
      "a7a9a2eccbbc40cf973fdc5c7a961ed2",
      "6dd13591bbae4d2f8ecd8e5d5dabca3a",
      "dfe14336e8fd4eba9f1f5103d0464ded",
      "dc0cef5da9b34f96b7a68e2bbcd2aaea"
     ]
    },
    "id": "34988dbb",
    "outputId": "6c1f370a-6685-47d8-c07d-d8a53b9f6bef"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2757f8ef0979425f80fd570ef65dec8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7abb913e854aedb2f9f0c549098f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9333c939775c4ca4b58e84de10d6ad7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b38fc9f9f24418a6b669b8eb1806a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e336038f80e1465baaaa80c52290dda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7814dab679ec4c288b46aa7db7882300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7167dd7de51c4ca8b60bad9a8ba499a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: google/flan-t5-base\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "print(\"‚úÖ Model loaded:\", model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af9c81",
   "metadata": {
    "id": "a4af9c81"
   },
   "source": [
    "## 5Ô∏è‚É£ Ask questions (RAG pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lHniTUPW2wRW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHniTUPW2wRW",
    "outputId": "0ad602c6-0e0d-4a85-a506-452a816d8d84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Folder ready with files: ['rag_intro.txt', 'best_practices.txt', 'vector_stores.txt']\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "os.makedirs(\"data_rag_demo\", exist_ok=True)\n",
    "for f in [\"rag_intro.txt\", \"vector_stores.txt\", \"best_practices.txt\"]:\n",
    "    if os.path.exists(f):\n",
    "        shutil.move(f, \"data_rag_demo/\")\n",
    "print(\"‚úÖ Folder ready with files:\", os.listdir(\"data_rag_demo\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0540d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73c0540d",
    "outputId": "b3166166-1aa8-4c17-803e-b6d2e812ed9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-708094551.py:13: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is RAG and why do we use a vector store?\n",
      "\n",
      "Context used:\n",
      " Good RAG design uses chunk overlaps, stores metadata for traceability, and evaluates answers for faithfulness to the retrieved context.\n",
      "Vector stores like Chroma store text embeddings so we can search by semantic meaning, not just exact keywords.\n",
      "Retrieval-Augmented Generation (RAG) retrieves relevant context from a knowledge base and passes it to a language model for grounded answers. ...\n",
      "\n",
      "A: retrievs relevant context from a knowledge base and passes it to a language model for grounded answers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_answer_from_context(question, context_text):\n",
    "    prompt = (\n",
    "        \"Use ONLY the context below to answer the question. \"\n",
    "        \"If the answer is not in the context, say you don't know.\\n\\n\"\n",
    "        f\"Context:\\n{context_text}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=256, temperature=0.0)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def rag_query(question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\".join([d.page_content for d in docs])\n",
    "    answer = generate_answer_from_context(question, context)\n",
    "    print(\"Q:\", question)\n",
    "    print(\"\\nContext used:\\n\", context[:500], \"...\\n\")\n",
    "    print(\"A:\", answer)\n",
    "\n",
    "rag_query(\"What is RAG and why do we use a vector store?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hGUsxEfd908I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "hGUsxEfd908I",
    "outputId": "4f6b9df7-8e05-489c-8a1f-4f1e6884cee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is RAG and why do we use a vector store?\n",
      "\n",
      "Context used:\n",
      " Good RAG design uses chunk overlaps, stores metadata for traceability, and evaluates answers for faithfulness to the retrieved context.\n",
      "Vector stores like Chroma store text embeddings so we can search by semantic meaning, not just exact keywords.\n",
      "Retrieval-Augmented Generation (RAG) retrieves relevant context from a knowledge base and passes it to a language model for grounded answers. ...\n",
      "\n",
      "A: retrievs relevant context from a knowledge base and passes it to a language model for grounded answers\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'retrievs relevant context from a knowledge base and passes it to a language model for grounded answers'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_answer_from_context(question, context_text):\n",
    "    prompt = (\n",
    "        \"Use ONLY the context below to answer the question. \"\n",
    "        \"If the answer is not in the context, say you don't know.\\n\\n\"\n",
    "        f\"Context:\\n{context_text}\\n\\n\"\n",
    "        f\"Question: {question}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=256,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def rag_query(question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "    answer = generate_answer_from_context(question, context)\n",
    "\n",
    "    print(\"Q:\", question)\n",
    "    print(\"\\nContext used:\\n\", context[:600], \"...\\n\")\n",
    "    print(\"A:\", answer)\n",
    "\n",
    "    return answer\n",
    "\n",
    "rag_query(\"What is RAG and why do we use a vector store?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df_30HoDKq7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6df_30HoDKq7",
    "outputId": "ebf5206f-9d8b-4fa2-fa54-25cfd3eff7ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Assuming current notebook is: RAG_Assignment.ipynb\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "import json, requests, re, os\n",
    "\n",
    "# This block grabs the current notebook name from the Colab UI:\n",
    "notebook_name = \"RAG_Assignment.ipynb\"  # <-- if you already named it something, put that name here\n",
    "\n",
    "print(\"‚úÖ Assuming current notebook is:\", notebook_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ivTIXQ4nFeTH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "ivTIXQ4nFeTH",
    "outputId": "23e3cc60-6a01-40cd-cbae-e46bd5bcfb72"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'RAG_Assignment.ipynb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nbformat/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(fp, as_version, capture_validation_error, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1868463023.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 1. Load the notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 2. Remove bad widget metadata from notebook-level metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nbformat/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(fp, as_version, capture_validation_error, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: PTH123\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_validation_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'RAG_Assignment.ipynb'"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "import json\n",
    "\n",
    "bad_path = \"RAG_Assignment.ipynb\"          # <-- your current notebook\n",
    "clean_path = \"RAG_Assignment_CLEAN.ipynb\"  # <-- we'll create this cleaned file\n",
    "\n",
    "# 1. Load the notebook\n",
    "nb = nbformat.read(bad_path, as_version=4)\n",
    "\n",
    "# 2. Remove bad widget metadata from notebook-level metadata\n",
    "if \"widgets\" in nb.metadata:\n",
    "    # GitHub expects a \"state\" field inside widgets metadata\n",
    "    widgets_meta = nb.metadata[\"widgets\"]\n",
    "    if not isinstance(widgets_meta, dict) or \"state\" not in widgets_meta:\n",
    "        # if it's malformed, just delete it\n",
    "        del nb.metadata[\"widgets\"]\n",
    "\n",
    "# 3. Remove widget metadata from each cell too (some cells carry their own junk)\n",
    "for cell in nb.cells:\n",
    "    if \"metadata\" in cell:\n",
    "        # Colab sometimes leaves \"id\", \"colab\", \"outputId\" etc., those are safe.\n",
    "        # We're only scrubbing broken widget metadata.\n",
    "        if \"widgets\" in cell.get(\"metadata\", {}):\n",
    "            w = cell[\"metadata\"][\"widgets\"]\n",
    "            if not isinstance(w, dict) or \"state\" not in w:\n",
    "                del cell[\"metadata\"][\"widgets\"]\n",
    "\n",
    "# 4. Save cleaned notebook\n",
    "nbformat.write(nb, clean_path)\n",
    "\n",
    "print(\"‚úÖ Cleaned notebook written to\", clean_path)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
